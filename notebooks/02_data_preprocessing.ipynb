{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e22462a",
   "metadata": {},
   "source": [
    "# Data Preprocessing - Census Income Prediction\n",
    "\n",
    "This notebook performs data preprocessing steps:\n",
    "- Remove duplicate rows\n",
    "- Fill null values\n",
    "- Treat outliers\n",
    "- Make label binary (0, 1)\n",
    "- Optional: Encode categoricals (one-hot encoding)\n",
    "- Feature engineering\n",
    "- Split into train, validation, and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e51a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "from src.data_loading import load_data, get_column_names\n",
    "from src.data_preprocessing import (\n",
    "    remove_duplicates,\n",
    "    handle_missing_values,\n",
    "    treat_outliers,\n",
    "    make_label_binary,\n",
    "    encode_categorical,\n",
    "    feature_engineering,\n",
    "    split_train_val_test\n",
    ")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb947a",
   "metadata": {},
   "source": [
    "## 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1bfa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training data shape: (199523, 42)\n",
      "Test data shape: (99762, 42)\n",
      "\n",
      "Target column: income\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>class_of_worker</th>\n",
       "      <th>detailed_industry_recode</th>\n",
       "      <th>detailed_occupation_recode</th>\n",
       "      <th>education</th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>enroll_in_edu_inst_last_wk</th>\n",
       "      <th>marital_stat</th>\n",
       "      <th>major_industry_code</th>\n",
       "      <th>major_occupation_code</th>\n",
       "      <th>race</th>\n",
       "      <th>hispanic_origin</th>\n",
       "      <th>sex</th>\n",
       "      <th>member_of_labor_union</th>\n",
       "      <th>reason_for_unemployment</th>\n",
       "      <th>full_or_part_time_employment_stat</th>\n",
       "      <th>capital_gains</th>\n",
       "      <th>capital_losses</th>\n",
       "      <th>dividends_from_stocks</th>\n",
       "      <th>tax_filer_stat</th>\n",
       "      <th>region_of_previous_residence</th>\n",
       "      <th>state_of_previous_residence</th>\n",
       "      <th>detailed_household_and_family_stat</th>\n",
       "      <th>detailed_household_summary_in_household</th>\n",
       "      <th>instance_weight</th>\n",
       "      <th>migration_code_change_in_msa</th>\n",
       "      <th>migration_code_change_in_reg</th>\n",
       "      <th>migration_code_move_within_reg</th>\n",
       "      <th>live_in_this_house_1_year_ago</th>\n",
       "      <th>migration_prev_res_in_sunbelt</th>\n",
       "      <th>num_persons_worked_for_employer</th>\n",
       "      <th>family_members_under_18</th>\n",
       "      <th>country_of_birth_father</th>\n",
       "      <th>country_of_birth_mother</th>\n",
       "      <th>country_of_birth_self</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>own_business_or_self_employed</th>\n",
       "      <th>fill_inc_questionnaire_for_veterans_admin</th>\n",
       "      <th>veterans_benefits</th>\n",
       "      <th>weeks_worked_in_year</th>\n",
       "      <th>year</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>High school graduate</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>White</td>\n",
       "      <td>All other</td>\n",
       "      <td>Female</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in labor force</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonfiler</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Other Rel 18+ ever marr not in subfamily</td>\n",
       "      <td>Other relative of householder</td>\n",
       "      <td>1700.09</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Not in universe under 1 year old</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Self-employed-not incorporated</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>Some college but no degree</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Precision production craft &amp; repair</td>\n",
       "      <td>White</td>\n",
       "      <td>All other</td>\n",
       "      <td>Male</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Children or Armed Forces</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Head of household</td>\n",
       "      <td>South</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Householder</td>\n",
       "      <td>Householder</td>\n",
       "      <td>1053.55</td>\n",
       "      <td>MSA to MSA</td>\n",
       "      <td>Same county</td>\n",
       "      <td>Same county</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10th grade</td>\n",
       "      <td>0</td>\n",
       "      <td>High school</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Asian or Pacific Islander</td>\n",
       "      <td>All other</td>\n",
       "      <td>Female</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in labor force</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonfiler</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Child 18+ never marr Not in a subfamily</td>\n",
       "      <td>Child 18 or older</td>\n",
       "      <td>991.95</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Not in universe under 1 year old</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Foreign born- Not a citizen of U S</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Children</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>White</td>\n",
       "      <td>All other</td>\n",
       "      <td>Female</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Children or Armed Forces</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonfiler</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Child &lt;18 never marr not in subfamily</td>\n",
       "      <td>Child under 18 never married</td>\n",
       "      <td>1758.14</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>Both parents present</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Children</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>White</td>\n",
       "      <td>All other</td>\n",
       "      <td>Female</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Children or Armed Forces</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonfiler</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Child &lt;18 never marr not in subfamily</td>\n",
       "      <td>Child under 18 never married</td>\n",
       "      <td>1069.16</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>Both parents present</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                  class_of_worker  detailed_industry_recode  \\\n",
       "0   73                  Not in universe                         0   \n",
       "1   58   Self-employed-not incorporated                         4   \n",
       "2   18                  Not in universe                         0   \n",
       "3    9                  Not in universe                         0   \n",
       "4   10                  Not in universe                         0   \n",
       "\n",
       "   detailed_occupation_recode                    education  wage_per_hour  \\\n",
       "0                           0         High school graduate              0   \n",
       "1                          34   Some college but no degree              0   \n",
       "2                           0                   10th grade              0   \n",
       "3                           0                     Children              0   \n",
       "4                           0                     Children              0   \n",
       "\n",
       "  enroll_in_edu_inst_last_wk    marital_stat           major_industry_code  \\\n",
       "0            Not in universe         Widowed   Not in universe or children   \n",
       "1            Not in universe        Divorced                  Construction   \n",
       "2                High school   Never married   Not in universe or children   \n",
       "3            Not in universe   Never married   Not in universe or children   \n",
       "4            Not in universe   Never married   Not in universe or children   \n",
       "\n",
       "                  major_occupation_code                        race  \\\n",
       "0                       Not in universe                       White   \n",
       "1   Precision production craft & repair                       White   \n",
       "2                       Not in universe   Asian or Pacific Islander   \n",
       "3                       Not in universe                       White   \n",
       "4                       Not in universe                       White   \n",
       "\n",
       "  hispanic_origin      sex member_of_labor_union reason_for_unemployment  \\\n",
       "0       All other   Female       Not in universe         Not in universe   \n",
       "1       All other     Male       Not in universe         Not in universe   \n",
       "2       All other   Female       Not in universe         Not in universe   \n",
       "3       All other   Female       Not in universe         Not in universe   \n",
       "4       All other   Female       Not in universe         Not in universe   \n",
       "\n",
       "  full_or_part_time_employment_stat  capital_gains  capital_losses  \\\n",
       "0                Not in labor force              0               0   \n",
       "1          Children or Armed Forces              0               0   \n",
       "2                Not in labor force              0               0   \n",
       "3          Children or Armed Forces              0               0   \n",
       "4          Children or Armed Forces              0               0   \n",
       "\n",
       "   dividends_from_stocks      tax_filer_stat region_of_previous_residence  \\\n",
       "0                      0            Nonfiler              Not in universe   \n",
       "1                      0   Head of household                        South   \n",
       "2                      0            Nonfiler              Not in universe   \n",
       "3                      0            Nonfiler              Not in universe   \n",
       "4                      0            Nonfiler              Not in universe   \n",
       "\n",
       "  state_of_previous_residence         detailed_household_and_family_stat  \\\n",
       "0             Not in universe   Other Rel 18+ ever marr not in subfamily   \n",
       "1                    Arkansas                                Householder   \n",
       "2             Not in universe    Child 18+ never marr Not in a subfamily   \n",
       "3             Not in universe      Child <18 never marr not in subfamily   \n",
       "4             Not in universe      Child <18 never marr not in subfamily   \n",
       "\n",
       "  detailed_household_summary_in_household  instance_weight  \\\n",
       "0           Other relative of householder          1700.09   \n",
       "1                             Householder          1053.55   \n",
       "2                       Child 18 or older           991.95   \n",
       "3            Child under 18 never married          1758.14   \n",
       "4            Child under 18 never married          1069.16   \n",
       "\n",
       "  migration_code_change_in_msa migration_code_change_in_reg  \\\n",
       "0                            ?                            ?   \n",
       "1                   MSA to MSA                  Same county   \n",
       "2                            ?                            ?   \n",
       "3                     Nonmover                     Nonmover   \n",
       "4                     Nonmover                     Nonmover   \n",
       "\n",
       "  migration_code_move_within_reg      live_in_this_house_1_year_ago  \\\n",
       "0                              ?   Not in universe under 1 year old   \n",
       "1                    Same county                                 No   \n",
       "2                              ?   Not in universe under 1 year old   \n",
       "3                       Nonmover                                Yes   \n",
       "4                       Nonmover                                Yes   \n",
       "\n",
       "  migration_prev_res_in_sunbelt  num_persons_worked_for_employer  \\\n",
       "0                             ?                                0   \n",
       "1                           Yes                                1   \n",
       "2                             ?                                0   \n",
       "3               Not in universe                                0   \n",
       "4               Not in universe                                0   \n",
       "\n",
       "  family_members_under_18 country_of_birth_father country_of_birth_mother  \\\n",
       "0         Not in universe           United-States           United-States   \n",
       "1         Not in universe           United-States           United-States   \n",
       "2         Not in universe                 Vietnam                 Vietnam   \n",
       "3    Both parents present           United-States           United-States   \n",
       "4    Both parents present           United-States           United-States   \n",
       "\n",
       "  country_of_birth_self                           citizenship  \\\n",
       "0         United-States     Native- Born in the United States   \n",
       "1         United-States     Native- Born in the United States   \n",
       "2               Vietnam   Foreign born- Not a citizen of U S    \n",
       "3         United-States     Native- Born in the United States   \n",
       "4         United-States     Native- Born in the United States   \n",
       "\n",
       "   own_business_or_self_employed fill_inc_questionnaire_for_veterans_admin  \\\n",
       "0                              0                           Not in universe   \n",
       "1                              0                           Not in universe   \n",
       "2                              0                           Not in universe   \n",
       "3                              0                           Not in universe   \n",
       "4                              0                           Not in universe   \n",
       "\n",
       "   veterans_benefits  weeks_worked_in_year  year     income  \n",
       "0                  2                     0    95   - 50000.  \n",
       "1                  2                    52    94   - 50000.  \n",
       "2                  2                     0    95   - 50000.  \n",
       "3                  0                     0    94   - 50000.  \n",
       "4                  0                     0    94   - 50000.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train_path = \"../data/census_income_learn.csv\"\n",
    "test_path = \"../data/census_income_test.csv\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_df, test_df = load_data(train_path, test_path)\n",
    "\n",
    "# Get column names\n",
    "columns = get_column_names()\n",
    "train_df.columns = columns\n",
    "test_df.columns = columns\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget column: {columns[-1]}\")\n",
    "\n",
    "# Display first few rows\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10465753",
   "metadata": {},
   "source": [
    "## 2. Remove Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50e55dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REMOVE DUPLICATE ROWS\n",
      "================================================================================\n",
      "\n",
      "Before removal:\n",
      "Training data shape: (199523, 42)\n",
      "Test data shape: (99762, 42)\n",
      "\n",
      "After removal:\n",
      "Training data shape: (152896, 42)\n",
      "Test data shape: (78864, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"REMOVE DUPLICATE ROWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check duplicates before removal\n",
    "target_col = 'income'\n",
    "print(f\"\\nBefore removal:\")\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Remove duplicates excluding instance_weight (survey weights may cause legitimate duplicates)\n",
    "# Note: See EDA notebook Section 4 for detailed explanation of why instance_weight causes duplicates\n",
    "columns_without_weight = [col for col in train_df.columns if col != 'instance_weight']\n",
    "\n",
    "train_df = remove_duplicates(train_df, subset=columns_without_weight, keep='first')\n",
    "test_df = remove_duplicates(test_df, subset=columns_without_weight, keep='first')\n",
    "\n",
    "print(f\"\\nAfter removal:\")\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a59e5f",
   "metadata": {},
   "source": [
    "## 3. Fill Null Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ab1e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILL NULL VALUES\n",
      "================================================================================\n",
      "\n",
      "Remaining nulls in training data: 0\n",
      "Remaining nulls in test data: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FILL NULL VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Handle missing values (replace '?' with NaN and fill)\n",
    "# Categorical columns: fill with \"not identified\"\n",
    "# Numerical columns: fill with median (or specify numerical_fill_value, e.g., -1)\n",
    "train_df = handle_missing_values(\n",
    "    train_df,\n",
    "    strategy='mode',  # Fill categorical with \"not identified\", numerical with median\n",
    "    missing_indicators=[\"?\", \" ?\", \"? \", \" ? \"],\n",
    "    categorical_fill_value=\"not identified\",\n",
    "    numerical_fill_value=None  # None = use median, or specify a value like -1\n",
    ")\n",
    "\n",
    "test_df = handle_missing_values(\n",
    "    test_df,\n",
    "    strategy='mode',\n",
    "    missing_indicators=[\"?\", \" ?\", \"? \", \" ? \"],\n",
    "    categorical_fill_value=\"not identified\",\n",
    "    numerical_fill_value=None  # None = use median, or specify a value like -1\n",
    ")\n",
    "\n",
    "# Check remaining nulls\n",
    "print(f\"\\nRemaining nulls in training data: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"Remaining nulls in test data: {test_df.isnull().sum().sum()}\")\n",
    "\n",
    "if train_df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nColumns with nulls in training data:\")\n",
    "    print(train_df.isnull().sum()[train_df.isnull().sum() > 0])\n",
    "\n",
    "if test_df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nColumns with nulls in test data:\")\n",
    "    print(test_df.isnull().sum()[test_df.isnull().sum() > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e8604",
   "metadata": {},
   "source": [
    "## 4. Treat Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f110789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TREAT OUTLIERS\n",
      "================================================================================\n",
      "\n",
      "Numerical columns to treat: 12\n",
      "Columns: ['age', 'detailed_industry_recode', 'detailed_occupation_recode', 'wage_per_hour', 'capital_gains', 'capital_losses', 'dividends_from_stocks', 'num_persons_worked_for_employer', 'own_business_or_self_employed', 'veterans_benefits']...\n",
      "\n",
      "Outliers treated using winsorization (1st-99th percentiles)\n",
      "Bounds calculated from training data and applied to both train and test\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TREAT OUTLIERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify numerical columns (excluding target and instance_weight)\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col not in [target_col, 'instance_weight']]\n",
    "\n",
    "print(f\"\\nNumerical columns to treat: {len(numerical_cols)}\")\n",
    "print(f\"Columns: {numerical_cols[:10]}...\" if len(numerical_cols) > 10 else f\"Columns: {numerical_cols}\")\n",
    "\n",
    "# Treat outliers using winsorization (clip to 1st and 99th percentiles)\n",
    "# Calculate bounds from training data, then apply same bounds to test data\n",
    "train_df, outlier_bounds = treat_outliers(\n",
    "    train_df,\n",
    "    columns=numerical_cols,\n",
    "    method='winsorize',\n",
    "    lower_percentile=0.01,\n",
    "    upper_percentile=0.99\n",
    ")\n",
    "\n",
    "# Apply same bounds from training data to test data\n",
    "test_df, _ = treat_outliers(\n",
    "    test_df,\n",
    "    columns=numerical_cols,\n",
    "    method='winsorize',\n",
    "    bounds=outlier_bounds\n",
    ")\n",
    "\n",
    "print(\"\\nOutliers treated using winsorization (1st-99th percentiles)\")\n",
    "print(\"Bounds calculated from training data and applied to both train and test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da81f01-bc28-4aec-a0ff-01eb8920e0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      - 50000.\n",
       "57      50000+.\n",
       "Name: income, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[target_col].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ee46f",
   "metadata": {},
   "source": [
    "## 5. Make Label Binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfe6fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MAKE LABEL BINARY\n",
      "================================================================================\n",
      "\n",
      "Training target distribution:\n",
      "income\n",
      "- 50000.    140529\n",
      "50000+.      12367\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test target distribution:\n",
      "income\n",
      "- 50000.    72678\n",
      "50000+.      6186\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After binary conversion:\n",
      "Training target distribution:\n",
      "income\n",
      "0    140529\n",
      "1     12367\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test target distribution:\n",
      "income\n",
      "0    72678\n",
      "1     6186\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MAKE LABEL BINARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check current target distribution\n",
    "print(f\"\\nTraining target distribution:\")\n",
    "print(train_df[target_col].value_counts())\n",
    "print(f\"\\nTest target distribution:\")\n",
    "print(test_df[target_col].value_counts())\n",
    "\n",
    "# Convert to binary (50000+. -> 1, -50000. -> 0)\n",
    "train_df = make_label_binary(train_df, target_column=target_col, positive_class='50000+.')\n",
    "test_df = make_label_binary(test_df, target_column=target_col, positive_class='50000+.')\n",
    "\n",
    "# Check binary target distribution\n",
    "print(f\"\\nAfter binary conversion:\")\n",
    "print(f\"Training target distribution:\")\n",
    "print(train_df[target_col].value_counts())\n",
    "print(f\"\\nTest target distribution:\")\n",
    "print(test_df[target_col].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3198a",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c92464ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "After feature engineering:\n",
      "Training data shape: (152896, 49)\n",
      "Test data shape: (78864, 49)\n",
      "\n",
      "New features created: 7\n",
      "Features: ['has_capital_gains', 'has_capital_losses', 'has_dividends', 'has_wage', 'age_group', 'total_financial_assets', 'work_intensity']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Apply feature engineering\n",
    "train_df = feature_engineering(train_df, target_column=target_col)\n",
    "test_df = feature_engineering(test_df, target_column=target_col)\n",
    "\n",
    "print(f\"\\nAfter feature engineering:\")\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Show new features created\n",
    "new_features = [col for col in train_df.columns if col not in columns]\n",
    "if new_features:\n",
    "    print(f\"\\nNew features created: {len(new_features)}\")\n",
    "    print(f\"Features: {new_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0e21f",
   "metadata": {},
   "source": [
    "## 7. Encode Categoricals (One-Hot Encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16764e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After encoding:\n",
      "Training data shape: (152896, 49)\n",
      "Test data shape: (78864, 49)\n",
      "\n",
      "Total features: 48\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ENCODE CATEGORICALS (ONE-HOT ENCODING)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Identify categorical columns (excluding target)\n",
    "    categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col != target_col]\n",
    "    \n",
    "    print(f\"\\nCategorical columns to encode: {len(categorical_cols)}\")\n",
    "    print(f\"Columns: {categorical_cols[:10]}...\" if len(categorical_cols) > 10 else f\"Columns: {categorical_cols}\")\n",
    "    \n",
    "    # Apply one-hot encoding to training data\n",
    "    train_df_encoded, _ = encode_categorical(\n",
    "        train_df,\n",
    "        method='onehot',\n",
    "        columns=categorical_cols\n",
    "    )\n",
    "    \n",
    "    # Apply one-hot encoding to test data\n",
    "    # Note: Test data might have different categories, so we need to align columns\n",
    "    test_df_encoded, _ = encode_categorical(\n",
    "        test_df,\n",
    "        method='onehot',\n",
    "        columns=categorical_cols\n",
    "    )\n",
    "    \n",
    "    # Align columns (add missing columns with zeros, remove extra columns)\n",
    "    train_cols = set(train_df_encoded.columns)\n",
    "    test_cols = set(test_df_encoded.columns)\n",
    "    \n",
    "    # Add missing columns to test (fill with 0)\n",
    "    missing_in_test = train_cols - test_cols\n",
    "    for col in missing_in_test:\n",
    "        if col != target_col:\n",
    "            test_df_encoded[col] = 0\n",
    "    \n",
    "    # Remove extra columns from test\n",
    "    extra_in_test = test_cols - train_cols\n",
    "    for col in extra_in_test:\n",
    "        if col != target_col:\n",
    "            test_df_encoded = test_df_encoded.drop(columns=[col])\n",
    "    \n",
    "    # Ensure same column order\n",
    "    test_df_encoded = test_df_encoded[train_df_encoded.columns]\n",
    "else:\n",
    "    train_df_encoded = train_df\n",
    "    test_df_encoded = test_df\n",
    "    \n",
    "print(f\"\\nAfter encoding:\")\n",
    "print(f\"Training data shape: {train_df_encoded.shape}\")\n",
    "print(f\"Test data shape: {test_df_encoded.shape}\")\n",
    "print(f\"\\nTotal features: {len(train_df_encoded.columns) - 1}\")  # Excluding target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4232c",
   "metadata": {},
   "source": [
    "## 8. Split into Train, Validation, and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ae3bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPLIT INTO TRAIN, VALIDATION, AND TEST SETS, the original learn csv is split while the test file is maintained\n",
      "================================================================================\n",
      "\n",
      "Data splits:\n",
      "Training set: 122316 samples, 48 features\n",
      "Validation set: 30580 samples, 48 features\n",
      "Test set: 78864 samples, 48 features\n",
      "\n",
      "Target distribution:\n",
      "Training: {0: 112422, 1: 9894}\n",
      "Validation: {0: 28107, 1: 2473}\n",
      "Test: {0: 72678, 1: 6186}\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SPLIT INTO TRAIN, VALIDATION, AND TEST SETS, the original learn csv is split while the test file is maintained\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_train_val_test(\n",
    "    train_df_encoded,\n",
    "    test_df_encoded,\n",
    "    target_column=target_col,\n",
    "    val_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples, {X_val.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"Training: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Validation: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"Test: {y_test.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20b14e",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7062cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVE PROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "Processed data saved to:\n",
      "  - ../data/processed/train_processed.csv\n",
      "  - ../data/processed/val_processed.csv\n",
      "  - ../data/processed/test_processed.csv\n",
      "\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAVE PROCESSED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Combine features and targets for saving\n",
    "train_processed = pd.concat([X_train, y_train], axis=1)\n",
    "val_processed = pd.concat([X_val, y_val], axis=1)\n",
    "test_processed = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "train_path = output_dir / \"train_processed.csv\"\n",
    "val_path = output_dir / \"val_processed.csv\"\n",
    "test_path = output_dir / \"test_processed.csv\"\n",
    "\n",
    "train_processed.to_csv(train_path, index=False)\n",
    "val_processed.to_csv(val_path, index=False)\n",
    "test_processed.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"\\nProcessed data saved to:\")\n",
    "print(f\"  - {train_path}\")\n",
    "print(f\"  - {val_path}\")\n",
    "print(f\"  - {test_path}\")\n",
    "\n",
    "print(f\"\\nPreprocessing complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
