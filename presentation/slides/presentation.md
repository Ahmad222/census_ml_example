# Census Income Prediction
## Machine Learning Project

**Dataiku Technical Assessment**  
**November 2025**

---

## Problem Formulation

### Objective
**Predict if income > $50,000/year**

üìä **Dataset:**
- ~300K individuals
- 40+ demographic & employment features
- Binary classification task

![Target Distribution](../../results/figures/target_distribution.png)




---

## Dataset Overview

üìà **Scale:**
- Training: ~200K samples
- Test: ~100K samples
- 41 features (13 numerical, 29 categorical)

‚ö†Ô∏è **Challenge:**
- **Severe class imbalance** (6.21% positive class)
- Requires specialized approaches

![Target Distribution](../../results/figures/target_distribution.png)

---

## Exploratory Data Analysis

### Data Quality Assessment

‚úÖ **Clean dataset** (~200K samples)  
‚úÖ **Missing values** well-documented  
‚ö†Ô∏è **Outliers** in financial features

![Missing Values](../../results/figures/missing_values.png)

---

## Exploratory Data Analysis

### Strong Predictors Identified

üéØ **Top Features:**
- Education level
- Occupation & Industry  
- Weeks worked
- Age
- Marital status

![Education vs Target](../../results/figures/education_vs_target.png)



![Target Distribution](../../results/figures/age_vs_target.png)


---

## Exploratory Data Analysis

### Feature Importance

üìä **Categorical Features:**
- Education, Occupation, Industry ‚Üí Strongest associations
- Marital status, Class of worker ‚Üí Highly predictive

![Categorical Feature Importance](../../results/figures/categorical_feature_importance.png)

---

## Exploratory Data Analysis

### Numerical Features

üìà **Strong Correlations:**
- Weeks worked ‚Üí Strong positive
- Age ‚Üí Moderate positive
- Financial features ‚Üí Highly predictive when non-zero

![Feature Correlation](../../results/figures/feature_correlation_target.png)

---

## Approach: Data Preprocessing

### Preprocessing Steps

1. **Remove Duplicates** (54K duplicates)
2. **Handle Missing Values** (categorical: "not identified", numerical: median)
3. **Treat Outliers** (winsorization)
4. **Feature Engineering** (6 new features)
5. **Feature Selection** (top 30 + engineered)
6. **Encode Categoricals** (hybrid: one-hot/frequency)

![Outliers](../../results/figures/outliers.png)

---

## Approach: Modeling

### Models
- **LightGBM** 
- **Random Forest**

### Hyperparameter Tuning
- **70 trials** (50 random + 20 TPE)
- Bayesian optimization (Optuna)
- Optimize for **ROC-AUC**

### Class Imbalance
- Class weights applied
- ROC-AUC as primary metric

---

## Approach: Evaluation

### Strategy
- **Train/Val/Test** split
- **ROC-AUC** primary metric
- **Comprehensive visualization**

### Metrics
- ROC-AUC, Precision, Recall, F1
- ROC & PR curves
- Confusion matrix

---

## Results: LightGBM Optimization

### Optuna Tuning History

**70 trials** ‚Üí Best hyperparameters selected

![LightGBM Optuna](../../results/figures/lgbm_optuna.png)


---

## Results: LightGBM Performance

### Performance Dashboard

![LightGBM Performance](../../results/figures/lgbm_model_performance.png)


---

## Results: Random Forest Optimization

### Optuna Tuning History

**70 trials** ‚Üí Best hyperparameters selected

![Random Forest Optuna](../../results/figures/rf_optuna.png)

---

## Results: Random Forest Performance

### Performance Dashboard

![Random Forest Performance](../../results/figures/rf_model_performance.png)

**Comprehensive evaluation:**
- Metrics comparison (Precision, Recall, F1, ROC-AUC)
- ROC & PR curves
- Confusion matrix

---


## Model Comparison: LightGBM vs Random Forest

### Key Differences

**LightGBM:**
- ‚úÖ Native categorical feature handling
- ‚úÖ Faster training & inference
- ‚úÖ Gradient boosting (sequential learning)
- ‚úÖ Efficient memory usage

**Random Forest:**
- ‚úÖ Ensemble of independent trees
- ‚úÖ More interpretable
- ‚úÖ Less prone to overfitting
- ‚úÖ Robust to outliers

---

### Trade-offs
- **Speed:** LightGBM faster
- **Interpretability:** Random Forest more interpretable
- **Performance:** Both achieve strong results, LGBM showed better generalization power in our example.

---

## Results: Key Insights

‚úÖ **Good generalization** (small train-val gap, especially in LGBM)  
‚úÖ **Strong discrimination** (high ROC-AUC)  
‚úÖ **Effective imbalance handling**

**Top Predictors:**
- Education, Occupation
- Financial features (when non-zero)
- Weeks worked, Age

---

## Next Steps

### High Priority

1. **Ensemble Methods**  
   Combine LightGBM + Random Forest

2. **Feature Engineering**  
   Interaction features (age √ó education)

3. **Threshold Optimization**  
   Cost-sensitive learning

4. **Model Interpretability**  
   SHAP values, feature explanations

---

## Next Steps

### Medium Priority

5. **SMOTE** - Synthetic oversampling  
6. **Additional Models** - CatBoost, XGBoost  
7. **Advanced Feature Selection** - RFE, SelectFromModel  
8. **Nested CV** - More robust tuning

---

## Summary

### What We Built
‚úÖ **Comprehensive EDA**
‚úÖ **Robust Preprocessing** 
‚úÖ **Optimized Models** 
‚úÖ **Thorough Evaluation**

### Key Achievements
- Handled **severe class imbalance** (6.21%)
- Identified **strong predictors**
- Built **reproducible pipeline**
- Achieved **good model performance**
**Notebooks:** `notebooks/`

